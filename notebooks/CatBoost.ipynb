{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QRT ENS Challenge Data 2023 - Benchmark\n",
    "\n",
    "Version 1 - Boosting, Feature engeneering & XGBoost \n",
    "\n",
    "## Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "from joblib import dump\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données\n",
    "\n",
    "- `X_train` et `X_test` ont  $35$ colonnes qui représentent les même variables explicatives mais sur des périodes de temps différentes. \n",
    "\n",
    "- `X_train` et `Y_train` partagent la même colonne `ID` - chaque ligne a un ID unique associé à un jour et à un pays. \n",
    "\n",
    "- La variable cible `TARGET` de `Y_train` correspond à la variation de prix journalière des futures sur l'électricité (maturité 24h).\n",
    "\n",
    "- **On notera que certaines colonnes ont des valeurs manquantes**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After downloading the X_train/X_test/Y_train .csv files in your working directory:\n",
    "\n",
    "X_train = pd.read_csv('../data/X_train.csv')\n",
    "Y_train = pd.read_csv('../data/y_train.csv')\n",
    "X_test = pd.read_csv('../data/X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "The main goal here is to reconstruct some of the lost time dimension to create stationary features.\n",
    "\n",
    "In general we had added statistics, technical indicators, seasonality, clusters and bag of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slope(y):\n",
    "    return np.polyfit(range(len(y)), y, 1)[0] if len(y) > 0 else np.nan\n",
    "\n",
    "def calculate_ema(data, window):\n",
    "    return data.ewm(span=window, adjust=False).mean()\n",
    "\n",
    "def calculate_rsi(data, window=14):\n",
    "    delta = data.diff(1)\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def calculate_bollinger_bands(data, window=20):\n",
    "    sma = data.rolling(window=window).mean()\n",
    "    std = data.rolling(window=window).std()\n",
    "    \n",
    "    bollinger_upper = sma + (std * 2)\n",
    "    bollinger_lower = sma - (std * 2)\n",
    "    \n",
    "    return bollinger_upper, bollinger_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(X, scaler, training):\n",
    "    df = X.copy()\n",
    "    df['COUNTRY'] = df['COUNTRY'].map({'DE': 0, 'FR': 1})\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    # Temporal window\n",
    "    windows = [7, 30]\n",
    "\n",
    "    # Column to calculate statistic\n",
    "    variables = ['CONSUMPTION', 'GAS', 'COAL', 'HYDRO', 'NUCLEAR', 'SOLAR', 'WINDPOW', 'TEMP', 'RAIN', 'WIND']\n",
    "    \n",
    "    def slope(y):\n",
    "        return np.polyfit(range(len(y)), y, 1)[0] if len(y) > 0 else np.nan\n",
    "\n",
    "    for var in variables:\n",
    "        for window in windows:\n",
    "            for country in ['DE_', 'FR_']:\n",
    "                # Mean, Standard deviation, Median, Minimum, Maximum, Slope\n",
    "                df[f'{country}{var}_MEAN_{window}D'] = df[f'{country}{var}'].rolling(window=window).mean()\n",
    "                df[f'{country}{var}_STD_{window}D'] = df[f'{country}{var}'].rolling(window=window).std()\n",
    "                df[f'{country}{var}_MEDIAN_{window}D'] = df[f'{country}{var}'].rolling(window=window).median()\n",
    "                df[f'{country}{var}_MIN_{window}D'] = df[f'{country}{var}'].rolling(window=window).min()\n",
    "                df[f'{country}{var}_MAX_{window}D'] = df[f'{country}{var}'].rolling(window=window).max()\n",
    "                df[f'{country}{var}_SLOPE_{window}D'] = df[f'{country}{var}'].rolling(window=window).apply(slope, raw=True)\n",
    "\n",
    "                # Additional features based on the original request\n",
    "                if var in ['CONSUMPTION', 'GAS', 'COAL', 'HYDRO', 'NUCLEAR', 'SOLAR', 'WINDPOW']:  # For RSI and Bollinger Bands\n",
    "                    df[f'{country}{var}_RSI'] = calculate_rsi(df[f'{country}{var}'])\n",
    "                    df[f'{country}{var}_BB_UPPER'], df[f'{country}{var}_BB_LOWER'] = calculate_bollinger_bands(df[f'{country}{var}'])\n",
    "    \n",
    "    # Seasonality Features\n",
    "    days_in_year = 365.25\n",
    "    df['SIN_YEAR'] = np.sin(2 * np.pi * df['DAY_ID'] / days_in_year)\n",
    "    df['COS_YEAR'] = np.cos(2 * np.pi * df['DAY_ID'] / days_in_year)\n",
    "    \n",
    "    days_in_week = 7\n",
    "    df['SIN_WEEK'] = np.sin(2 * np.pi * df['DAY_ID'] / days_in_week)\n",
    "    df['COS_WEEK'] = np.cos(2 * np.pi * df['DAY_ID'] / days_in_week)\n",
    "\n",
    "    df['SEASON'] = pd.cut(df['DAY_ID'] % 365, \n",
    "                          bins=[0, 79, 172, 264, 365], \n",
    "                          labels=[0, 1, 2, 3],\n",
    "                          right=False).astype(int)\n",
    "    \n",
    "    for energy_source in ['GAS', 'COAL', 'HYDRO', 'NUCLEAR', 'SOLAR', 'WINDPOW', 'LIGNITE']:\n",
    "        df[f'DE_{energy_source}_RATIO'] = df[f'DE_{energy_source}'] / (df['DE_GAS'] + df['DE_COAL'] + df['DE_HYDRO'] + df['DE_NUCLEAR'] + df['DE_SOLAR'] + df['DE_WINDPOW'] + df['DE_LIGNITE'])\n",
    "\n",
    "    df['DE_IMPORT_RATIO'] = df['DE_NET_IMPORT'] / df['DE_CONSUMPTION']\n",
    "    df['DE_EXPORT_RATIO'] = df['DE_NET_EXPORT'] / (df['DE_GAS'] + df['DE_COAL'] + df['DE_HYDRO'] + df['DE_NUCLEAR'] + df['DE_SOLAR'] + df['DE_WINDPOW'] + df['DE_LIGNITE'])\n",
    "\n",
    "    df['DE_FR_NET_EXCHANGE'] = df['DE_FR_EXCHANGE'] - df['FR_DE_EXCHANGE']\n",
    "\n",
    "    df['DE_WIND_SOLAR'] = df['DE_WINDPOW'] + df['DE_SOLAR']\n",
    "    df['DE_TEMP_EFFECT'] = df['DE_TEMP'] * df['DE_CONSUMPTION']\n",
    "    df['DE_WIND_EFFECT'] = df['DE_WIND'] * df['DE_WINDPOW']\n",
    "    df['DE_SOLAR_EFFECT'] = (df['DE_SOLAR'] / df['DE_TEMP']).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    for commodity in ['GAS_RET', 'COAL_RET', 'CARBON_RET']:\n",
    "        df[f'{commodity}_VOLATILITY_7D'] = df[commodity].rolling(window=7).std()\n",
    "        df[f'{commodity}_VOLATILITY_30D'] = df[commodity].rolling(window=30).std()\n",
    "\n",
    "    for commodity in ['GAS_RET', 'COAL_RET', 'CARBON_RET']:\n",
    "        df[f'{commodity}_EMA_30D'] = df[commodity].ewm(span=30, adjust=False).mean()\n",
    "\n",
    "    df['DE_RESIDUAL_LOAD_RATIO'] = df['DE_RESIDUAL_LOAD'] / df['DE_CONSUMPTION']\n",
    "    df['DE_Imbalance'] = (df['DE_GAS'] + df['DE_COAL'] + df['DE_HYDRO'] + df['DE_NUCLEAR'] + df['DE_SOLAR'] + df['DE_WINDPOW'] + df['DE_LIGNITE']) - df['DE_CONSUMPTION'] - df['DE_NET_EXPORT']\n",
    "    \n",
    "    for energy_source in ['GAS', 'COAL', 'HYDRO', 'NUCLEAR', 'SOLAR', 'WINDPOW']:\n",
    "        df[f'FR_{energy_source}_RATIO'] = df[f'FR_{energy_source}'] / (df['FR_GAS'] + df['FR_COAL'] + df['FR_HYDRO'] + df['FR_NUCLEAR'] + df['FR_SOLAR'] + df['FR_WINDPOW'])\n",
    "\n",
    "    df['FR_IMPORT_RATIO'] = df['FR_NET_IMPORT'] / df['FR_CONSUMPTION']\n",
    "    df['FR_EXPORT_RATIO'] = df['FR_NET_EXPORT'] / (df['FR_GAS'] + df['FR_COAL'] + df['FR_HYDRO'] + df['FR_NUCLEAR'] + df['FR_SOLAR'] + df['FR_WINDPOW'])\n",
    "\n",
    "    df['FR_WIND_SOLAR'] = df['FR_WINDPOW'] + df['FR_SOLAR']\n",
    "    df['FR_TEMP_EFFECT'] = df['FR_TEMP'] * df['FR_CONSUMPTION']\n",
    "    df['FR_WIND_EFFECT'] = df['FR_WIND'] * df['FR_WINDPOW']\n",
    "    df['FR_SOLAR_EFFECT'] = (df['FR_SOLAR'] / df['FR_TEMP']).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    df['FR_WIND_SOLAR'] = df['FR_WINDPOW'] + df['FR_SOLAR']\n",
    "    df['FR_TEMP_EFFECT'] = df['FR_TEMP'] * df['FR_CONSUMPTION']\n",
    "    df['FR_WIND_EFFECT'] = df['FR_WIND'] * df['FR_WINDPOW']\n",
    "    df['FR_SOLAR_EFFECT'] = (df['FR_SOLAR'] / df['FR_TEMP']).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    df['FR_RESIDUAL_LOAD_RATIO'] = df['FR_RESIDUAL_LOAD'] / df['FR_CONSUMPTION']\n",
    "    df['FR_Imbalance'] = (df['FR_GAS'] + df['FR_COAL'] + df['FR_HYDRO'] + df['FR_NUCLEAR'] + df['FR_SOLAR'] + df['FR_WINDPOW']) - df['FR_CONSUMPTION'] - df['FR_NET_EXPORT']\n",
    "    \n",
    "    df.fillna(method='bfill', inplace=True)\n",
    "    if training:\n",
    "        df_scaled = scaler.fit_transform(df)\n",
    "    else:\n",
    "        df_scaled = scaler.transform(df)\n",
    "    \n",
    "    # Adjust 'n_clusters' based on your dataset and experimentation\n",
    "    kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "    df['cluster'] = kmeans.fit_predict(df_scaled)\n",
    "    \n",
    "    # Creating bag of features based on cluster means/medians\n",
    "    for cluster in df['cluster'].unique():\n",
    "        cluster_mask = df['cluster'] == cluster\n",
    "        for var in variables:\n",
    "            for country in ['DE_', 'FR_']:\n",
    "                cluster_mean = df.loc[cluster_mask, country+var].mean()\n",
    "                cluster_median = df.loc[cluster_mask, country+var].median()\n",
    "                df[f'{country}{var}_cluster_{cluster}_mean_diff'] = df[country+var] - cluster_mean\n",
    "                df[f'{country}{var}_cluster_{cluster}_median_diff'] = df[country+var] - cluster_median\n",
    "    \n",
    "    # Différenciation par rapport à la moyenne mobile\n",
    "    for var in variables:\n",
    "        for country in ['DE_', 'FR_']:\n",
    "            df[f'{country}{var}_diff_ma2'] = df[country+var] - df[country+var].rolling(window=2).mean()\n",
    "            df[f'{country}{var}_diff_ma5'] = df[country+var] - df[country+var].rolling(window=5).mean()\n",
    "    \n",
    "    # Ensuring all missing data filled if any new were created\n",
    "    df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Apply the feature engineering to the df*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_featurized = feature_engineering(X_train, scaler, True)\n",
    "print(f\"Final number of columns: {len(X_featurized.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Check the stationarity of all features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(df):\n",
    "    results = {}\n",
    "    for column in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            adf_test = adfuller(df[column].dropna(), autolag='AIC')\n",
    "            results[column] = {'Test Statistic': adf_test[0], \n",
    "                               'p-value': adf_test[1], \n",
    "                               'Used Lag': adf_test[2], \n",
    "                               'Number of Observations Used': adf_test[3], \n",
    "                               'Critical Values': adf_test[4], \n",
    "                               'Stationary': adf_test[1] < 0.05}\n",
    "    return results\n",
    "\n",
    "def check_stationarity(stationarity_results):\n",
    "    non_stationary_features = {feature: result for feature, result in stationarity_results.items() if not result['Stationary']}\n",
    "    \n",
    "    if non_stationary_features:\n",
    "        print(\"Features non stationnaires détectées :\")\n",
    "        for feature, result in non_stationary_features.items():\n",
    "            print(f\"  Feature: {feature}\")\n",
    "            print(f\"    Test Statistic: {result['Test Statistic']}\")\n",
    "            print(f\"    p-value: {result['p-value']}\")\n",
    "            print(f\"    Stationary: {result['Stationary']}\")\n",
    "            print(\"\\n\")\n",
    "    else:\n",
    "        print(\"All features are stationary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_results = test_stationarity(X_featurized)\n",
    "check_stationarity(stationarity_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print random 9 features to check the stationarity\n",
    "selected_features = X_featurized.sample(9, axis=1).columns\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "fig.suptitle('Plot of 9 Features on an interval of 363 days')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "for i, feature in enumerate(selected_features):\n",
    "    row, col = i // 3, i % 3\n",
    "    axes[row, col].plot(X_featurized[feature].iloc[:363])\n",
    "    axes[row, col].set_title(feature)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Naive method without any hyperparameter optimization in cross validation in order to find best features*\n",
    "The general goal here is to get the best features in order to do the optimization of hyper parameter on a reduced space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor, Pool, cv\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy.stats import spearmanr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X = X_featurized\n",
    "Y = Y_train['TARGET'].rank(method='average')\n",
    "\n",
    "params = {\n",
    "    'loss_function': 'MAE',\n",
    "    'learning_rate': 0.1,\n",
    "    'depth': 6,\n",
    "    'iterations': 1000,\n",
    "    'verbose': 100,\n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "# Configuration de la validation croisée\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "feature_importances = pd.DataFrame(index=X.columns, columns=[\"Importance\"]).fillna(0)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "    train_pool = Pool(X_train, y_train)\n",
    "    test_pool = Pool(X_test, y_test)\n",
    "    \n",
    "    model = CatBoostRegressor(**params)\n",
    "    model.fit(train_pool, eval_set=test_pool, early_stopping_rounds=50, verbose=False)\n",
    "    \n",
    "    # Évaluation sur l'ensemble de test\n",
    "    predictions_test = model.predict(X_test)\n",
    "    spearman_corr_test = spearmanr(predictions_test, y_test).correlation\n",
    "    print(\"Spearman correlation for test data: {:.1f}%\".format(100 * spearman_corr_test))\n",
    "    \n",
    "    # Mise à jour correcte de l'importance des features\n",
    "    feature_importance_df = pd.DataFrame(model.get_feature_importance(), index=X.columns, columns=[\"Importance\"])\n",
    "    feature_importances += feature_importance_df / kf.n_splits\n",
    "\n",
    "print(\"Importance of features averaged over CV folds:\")\n",
    "print(feature_importances.sort_values(by=\"Importance\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = feature_importances.sort_values(by=\"Importance\", ascending=False).head(20)\n",
    "\n",
    "# Créer un histogramme des importances des top 20 features\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features['Importance'].plot(kind='barh')\n",
    "plt.title('Top 20 Features Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.gca().invert_yaxis()  # Inverser l'axe y pour afficher la feature la plus importante en haut\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use of GridSearch to find optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    }
   ],
   "source": [
    "X = X_featurized\n",
    "Y = Y_train['TARGET'].rank(method='average')\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'depth': [4, 6],\n",
    "    'l2_leaf_reg': [1, 3],\n",
    "}\n",
    "\n",
    "# Configuration du modèle pour la recherche d'hyperparamètres\n",
    "model = CatBoostRegressor(loss_function='MAE', iterations=200, verbose=False, random_state=42)\n",
    "\n",
    "# Création du scorer basé sur Spearman Correlation, puisque votre objectif est d'optimiser cela\n",
    "def spearman_scorer(y_true, y_pred):\n",
    "    return spearmanr(y_true, y_pred).correlation\n",
    "\n",
    "scorer = make_scorer(spearman_scorer, greater_is_better=True)\n",
    "\n",
    "# Configuration de GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring=scorer, verbose=2)\n",
    "\n",
    "# Recherche des meilleurs hyperparamètres sur l'ensemble d'entraînement\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Spearman correlation score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_score_)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best Spearman correlation score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "dump(best_model, '../models/best_catboost_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('../data/X_test.csv')\n",
    "y_test = pd.read_csv('../data/y_test.csv')\n",
    "\n",
    "y_test['TARGET_RANK'] = y_test['TARGET'].rank(method='average')\n",
    "\n",
    "X_featurized_test = feature_engineering(X_test, scaler, False)\n",
    "predictions_test = best_model.predict(X_featurized_test)\n",
    "\n",
    "Y_test_submission = X_test[['ID']].copy()\n",
    "Y_test_submission['TARGET'] = predictions_test\n",
    "Y_test_submission.to_csv('submission_catboost.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "e98505b2ea4c5ad54dad79b106a9e9e74f288112ea588ce88c6ce949430e0824"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
