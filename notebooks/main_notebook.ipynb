{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QRT ENS Challenge Data 2023 - Benchmark\n",
    "\n",
    "Version 1 - Boosting, Feature engeneering & XGBoost \n",
    "\n",
    "## Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données\n",
    "\n",
    "- `X_train` et `X_test` ont  $35$ colonnes qui représentent les même variables explicatives mais sur des périodes de temps différentes. \n",
    "\n",
    "- `X_train` et `Y_train` partagent la même colonne `ID` - chaque ligne a un ID unique associé à un jour et à un pays. \n",
    "\n",
    "- La variable cible `TARGET` de `Y_train` correspond à la variation de prix journalière des futures sur l'électricité (maturité 24h).\n",
    "\n",
    "- **On notera que certaines colonnes ont des valeurs manquantes**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After downloading the X_train/X_test/Y_train .csv files in your working directory:\n",
    "\n",
    "X_train = pd.read_csv('../data/raw/X_train.csv')\n",
    "Y_train = pd.read_csv('../data/raw/y_train.csv')\n",
    "X_test = pd.read_csv('../data/raw/X_test_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"TARGET\"] = Y_train[\"TARGET\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "The main goal here is to reconstruct some of the lost time dimension to create stationary features.\n",
    "\n",
    "In general we had added statistics, technical indicators, seasonality, clusters and bag of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slope(y):\n",
    "    return np.polyfit(range(len(y)), y, 1)[0] if len(y) > 0 else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_statistics(df, variables, windows, countries=['DE_', 'FR_']):\n",
    "    # Define a slope calculation function\n",
    "    def slope(y):\n",
    "        return np.polyfit(range(len(y)), y, 1)[0] if len(y) > 0 else np.nan\n",
    "\n",
    "    # Calculate rolling statistics for each variable and window\n",
    "    for var in variables:\n",
    "        for window in windows:\n",
    "            for country in countries:\n",
    "                df[f'{country}{var}_MEAN_{window}D'] = df[f'{country}{var}'].rolling(window=window).mean()\n",
    "                df[f'{country}{var}_STD_{window}D'] = df[f'{country}{var}'].rolling(window=window).std()\n",
    "                df[f'{country}{var}_MEDIAN_{window}D'] = df[f'{country}{var}'].rolling(window=window).median()\n",
    "                df[f'{country}{var}_MIN_{window}D'] = df[f'{country}{var}'].rolling(window=window).min()\n",
    "                df[f'{country}{var}_MAX_{window}D'] = df[f'{country}{var}'].rolling(window=window).max()\n",
    "                # Apply the slope function to the rolling window\n",
    "                df[f'{country}{var}_SLOPE_{window}D'] = df[f'{country}{var}'].rolling(window=window).apply(slope, raw=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_seasonality_features(df):\n",
    "    days_in_year = 365.25\n",
    "    df['SIN_YEAR'] = np.sin(2 * np.pi * df['DAY_ID'] / days_in_year)\n",
    "    df['COS_YEAR'] = np.cos(2 * np.pi * df['DAY_ID'] / days_in_year)\n",
    "    \n",
    "    days_in_week = 7\n",
    "    df['SIN_WEEK'] = np.sin(2 * np.pi * df['DAY_ID'] / days_in_week)\n",
    "    df['COS_WEEK'] = np.cos(2 * np.pi * df['DAY_ID'] / days_in_week)\n",
    "\n",
    "    df['SEASON'] = pd.cut(df['DAY_ID'] % 365, bins=[0, 79, 172, 264, 365], labels=[0, 1, 2, 3], right=False).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_energy_source_ratios_and_effects(df):\n",
    "    for country in ['DE_', 'FR_']:\n",
    "        for energy_source in ['GAS', 'COAL', 'HYDRO', 'NUCLEAR', 'SOLAR', 'WINDPOW']:\n",
    "            total_energy = df[f'{country}GAS'] + df[f'{country}COAL'] + df[f'{country}HYDRO'] + \\\n",
    "                           df[f'{country}NUCLEAR'] + df[f'{country}SOLAR'] + df[f'{country}WINDPOW']\n",
    "            df[f'{country}{energy_source}_RATIO'] = df[f'{country}{energy_source}'] / total_energy\n",
    "\n",
    "        df[f'{country}WIND_SOLAR'] = df[f'{country}WINDPOW'] + df[f'{country}SOLAR']\n",
    "        df[f'{country}TEMP_EFFECT'] = df[f'{country}TEMP'] * df[f'{country}CONSUMPTION']\n",
    "        df[f'{country}WIND_EFFECT'] = df[f'{country}WIND'] * df[f'{country}WINDPOW']\n",
    "        df[f'{country}SOLAR_EFFECT'] = (df[f'{country}SOLAR'] / df[f'{country}TEMP']).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_market_features(df):\n",
    "    for commodity in ['GAS_RET', 'COAL_RET', 'CARBON_RET']:\n",
    "        df[f'{commodity}_VOLATILITY_7D'] = df[commodity].rolling(window=7).std()\n",
    "        df[f'{commodity}_VOLATILITY_30D'] = df[commodity].rolling(window=30).std()\n",
    "        df[f'{commodity}_EMA_30D'] = df[commodity].ewm(span=30, adjust=False).mean()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_custom_features(df):\n",
    "    # Temporal window and variables for rolling statistics\n",
    "    windows = [7, 30]\n",
    "    variables = ['CONSUMPTION', 'GAS', 'COAL', 'HYDRO', 'NUCLEAR', 'SOLAR', 'WINDPOW', 'TEMP', 'RAIN', 'WIND']\n",
    "\n",
    "    # Calculate rolling statistics and other features\n",
    "    df = add_rolling_statistics(df, variables, windows)\n",
    "\n",
    "    # Seasonality Features\n",
    "    df = add_seasonality_features(df)\n",
    "\n",
    "    # Energy Source Ratios and Effects\n",
    "    df = add_energy_source_ratios_and_effects(df)\n",
    "\n",
    "    # Market Volatility and Moving Averages\n",
    "    df = add_market_features(df)\n",
    "\n",
    "    # Ensure all missing data are filled if any new were created\n",
    "    df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(X):\n",
    "    # Copy the original dataframe to avoid modifying it directly\n",
    "    df = X.copy()\n",
    "    \n",
    "    # Define the country codes\n",
    "    country_codes = {'DE': 0, 'FR': 1}\n",
    "    \n",
    "    # Map country codes to numerical values\n",
    "    df['COUNTRY'] = df['COUNTRY'].map(country_codes)\n",
    "\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df[df.columns] = imputer.fit_transform(df)\n",
    "\n",
    "    df = add_custom_features(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Apply the feature engineering to the df*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = feature_engineering(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_based_on_correlation(dataframe, target_column, multicollinear_threshold=0.7, correlation_threshold=0.05):\n",
    "    # Calculate the Spearman correlation matrix\n",
    "    corr_matrix = dataframe.corr(method='spearman')\n",
    "    \n",
    "    # Identify features that are highly correlated with each other\n",
    "    # (excluding the target variable correlation)\n",
    "    high_corr_var = np.where(corr_matrix > multicollinear_threshold)\n",
    "    high_corr_var = [(corr_matrix.index[x], corr_matrix.columns[y]) \n",
    "                     for x, y in zip(*high_corr_var) \n",
    "                     if x != y and x < y]\n",
    "    \n",
    "    # Extract the names of columns to drop based on multicollinearity\n",
    "    multicollinear_features = set([item for sublist in high_corr_var for item in sublist])\n",
    "    \n",
    "    # Identify features that have a low correlation with the target variable\n",
    "    low_corr_with_target = corr_matrix[target_column][abs(corr_matrix[target_column]) < correlation_threshold].index.tolist()\n",
    "    \n",
    "    # Combine features to drop due to multicollinearity and low correlation with target\n",
    "    features_to_drop = multicollinear_features.union(low_corr_with_target)\n",
    "    \n",
    "    # Determine the final list of features to keep\n",
    "    features_to_keep = [feature for feature in dataframe.columns if feature not in features_to_drop and feature != target_column]\n",
    "    \n",
    "    return features_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_features(features, max_features):\n",
    "    if len(features) > max_features:\n",
    "        return features[:max_features]\n",
    "    else:\n",
    "        return features\n",
    "\n",
    "selected_features = []\n",
    "\n",
    "refined_features = select_features_based_on_correlation(df, 'TARGET', 0.7, 0.05)\n",
    "top_features = select_top_features(refined_features, 30)\n",
    "df_reduced = df[top_features + ['TARGET']]\n",
    "print(f\"Selected features for df: {len(df_reduced.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use of GridSearch to find optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_catboost(X, y):\n",
    "    # Define the parameter grid to search over\n",
    "    param_grid = {\n",
    "        'depth': [4, 6],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'iterations': [200],\n",
    "        'l2_leaf_reg': [3, 7]\n",
    "    }\n",
    "\n",
    "    # Determine all combinations\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    # Set up K-Fold cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "\n",
    "    # Iterate over combinations\n",
    "    for params in combinations:\n",
    "        scores = []\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            model = CatBoostRegressor(silent=True, **params)\n",
    "            model.fit(X_train, y_train)\n",
    "            predictions = model.predict(X_test)\n",
    "            \n",
    "            score = spearmanr(y_test, predictions).correlation\n",
    "            scores.append(score)\n",
    "\n",
    "        mean_score = np.mean(scores)\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_params = params\n",
    "\n",
    "        print(f\"Params: {params}, Mean Spearman Correlation: {mean_score}\")\n",
    "\n",
    "    print(f\"Best Params: {best_params}, Best Mean Spearman Correlation: {best_score}\")\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training CatBoost model...\")\n",
    "X = df_reduced.drop('TARGET', axis=1)\n",
    "y = df_reduced['TARGET'].rank()\n",
    "best_params = train_and_evaluate_catboost(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain with best params on whole dataset\n",
    "final_model = CatBoostRegressor(silent=True, **best_params)\n",
    "final_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final = pd.read_csv('../data/raw/X_test_final.csv')\n",
    "df_test = feature_engineering(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_reduced = df_test[top_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = final_model.predict(df_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_submission = X_test_final[['ID']].copy()\n",
    "Y_test_submission[\"TARGET\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_submission.to_csv('submission_catboost.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "e98505b2ea4c5ad54dad79b106a9e9e74f288112ea588ce88c6ce949430e0824"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
